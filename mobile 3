import os
import re
import json
import shutil
from multiprocessing import Pool, Manager, cpu_count
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE   = "input.txt"
CHUNK_SIZE   = 2000
OUTPUT_DOCX  = "aerospike_mobiles.docx"
OUTPUT_XLSX  = "aerospike_mobiles.xlsx"
TEMP_DIR     = "temp_aero_parts"
MOBILE_REGEX = r'(?<!\d)((?:\+91[\-\s]?|91[\-\s]?|0)?[6-9]\d{9})(?!\d)'

def flatten_json(obj, prefix=""):
    flat = {}
    if isinstance(obj, dict):
        for k, v in obj.items():
            path = f"{prefix}{k}"
            if isinstance(v, (dict, list)):
                flat.update(flatten_json(v, path + "."))
            elif isinstance(v, str) and v.strip().startswith(("{", "[")):
                try:
                    inner = json.loads(v)
                except:
                    flat[path] = v
                else:
                    flat.update(flatten_json(inner, path + "."))
            else:
                flat[path] = v
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            flat.update(flatten_json(item, f"{prefix}[{i}]."))
    return flat

def load_records():
    """
    Load only headers containing 'JSON Data:', skip others silently.
    Warnings only for malformed JSON blocks or unbalanced braces.
    Returns records, parse_warnings, skipped_headers.
    """
    text = open(INPUT_FILE, encoding="utf-8", errors="ignore").read()
    # Header match up to JSON Data:
    header_pat = re.compile(
        r"Set\s+Name\s*:\s*(?P<set>[^,]+)"
        r"(?:\s*,\s*Key\s*:\s*(?P<key>[^,]+))?"
        r".*?JSON\s*Data\s*:",
        flags=re.IGNORECASE
    )
    records = []
    skipped_headers = []
    parse_warnings = []

    pos = 0
    while True:
        m = header_pat.search(text, pos)
        if not m:
            break
        set_name = m.group("set").strip()
        key = (m.group("key") or "").strip()
        json_start = text.find("{", m.end())
        if json_start < 0:
            skipped_headers.append((set_name, key, "no '{' after JSON Data"))
            pos = m.end()
            continue
        # brace scan
        brace = 1
        i = json_start + 1
        L = len(text)
        while i < L and brace:
            if text[i] == "{": brace += 1
            elif text[i] == "}": brace -= 1
            i += 1
        if brace != 0:
            snippet = text[json_start:json_start+200]
            parse_warnings.append((set_name, key, snippet))
            pos = m.end()
            continue
        raw_json = text[json_start:i]
        records.append((set_name, key, raw_json))
        pos = i
    return records, skipped_headers, parse_warnings

def chunk_records(records):
    for idx in range(0, len(records), CHUNK_SIZE):
        yield records[idx:idx+CHUNK_SIZE], idx//CHUNK_SIZE

def process_chunk(args):
    chunk, idx, result_list = args
    pat = re.compile(MOBILE_REGEX)
    doc = Document()
    rows = []
    for set_name, key, raw_json in chunk:
        # Always attempt mobile match in raw_json even if JSON parse fails
        try:
            obj = json.loads(raw_json)
            flat = flatten_json(obj)
            values = [json.dumps(v) if isinstance(v,(dict,list)) else str(v) for v in flat.values()]
        except:
            flat = {}
            values = []
        for m in pat.finditer(raw_json):
            num = m.group(1)
            # accept even if JSON failed
            # try to find field path
            field = ""
            for path, v in flat.items():
                txt = json.dumps(v) if isinstance(v,(dict,list)) else str(v)
                if num in txt:
                    field = path
                    break
            rows.append((set_name, key, raw_json, num, field))
    if rows:
        os.makedirs(TEMP_DIR, exist_ok=True)
        doc2 = Document()
        for set_name, key, raw_json, num, field in rows:
            para = doc2.add_paragraph(f"{set_name} | {key} | ")
            para.add_run(raw_json.replace('\n',' '))  # full context
            para.add_run(" | ").add_run(num).font.color.rgb = RGBColor(255,0,0)
            para.add_run(" | field: ").add_run(field).font.color.rgb = RGBColor(255,0,0)
        doc2.save(os.path.join(TEMP_DIR, f"chunk_{idx}.docx"))
    result_list.append(rows)

def merge_word():
    merged = Document()
    for fn in tqdm(sorted(os.listdir(TEMP_DIR)), desc="Merging Word"):
        sub = Document(os.path.join(TEMP_DIR,fn))
        for p in sub.paragraphs:
            np = merged.add_paragraph()
            for r in p.runs:
                nr=np.add_run(r.text)
                if r.font.color and r.font.color.rgb:
                    nr.font.color.rgb=r.font.color.rgb
                nr.bold,nr.italic,nr.underline = r.bold,r.italic,r.underline
    merged.save(OUTPUT_DOCX)

def write_excel(all_rows):
    wb=xlsxwriter.Workbook(OUTPUT_XLSX); ws=wb.add_worksheet()
    red=wb.add_format({'font_color':'red'})
    ws.write_row(0,0,['Set Name','Key','Full JSON','Mobile','Field'])
    r=1
    for a,b,c,m,f in all_rows:
        ws.write(r,0,a); ws.write(r,1,b); ws.write(r,2,c)
        ws.write(r,3,m,red); ws.write(r,4,f)
        r+=1
    wb.close()

if __name__=='__main__':
    if os.path.isdir(TEMP_DIR): shutil.rmtree(TEMP_DIR)
    print("Loading records…")
    recs, skipped_hdrs, parse_warns = load_records()
    print(f"Records loaded: {len(recs)}")
    if skipped_hdrs:
        print("Skipped headers:")
        for sn,k,why in skipped_hdrs: print(" ",sn,k,why)
    if parse_warns:
        print("JSON parse failures:")
        for sn,k,snip in parse_warns: print(" ",sn,k,snip,"\n")

    mgr=Manager(); results=mgr.list()
    chunks=list(chunk_records(recs))
    with Pool(min(cpu_count(),len(chunks))) as p:
        list(tqdm(p.imap_unordered(process_chunk,
             [(ch,i,results) for ch,i in chunks]),
             total=len(chunks),desc="Processing"))

    all_rows=[]
    for rows in results:
        all_rows.extend(rows)
    print(f"Total matches: {len(all_rows)}")

    if os.path.isdir(TEMP_DIR) and all_rows:
        merge_word()
    write_excel(all_rows)
    if os.path.isdir(TEMP_DIR): shutil.rmtree(TEMP_DIR,ignore_errors=True)
    print("Done →",OUTPUT_DOCX,OUTPUT_XLSX)
