import os
import re
import json
import shutil
from multiprocessing import Pool, Manager, cpu_count
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE   = "input.txt"
CHUNK_SIZE   = 2000
OUTPUT_DOCX  = "aerospike_mobiles.docx"
OUTPUT_XLSX  = "aerospike_mobiles.xlsx"
TEMP_DIR     = "temp_aero_parts"
MOBILE_REGEX = r'(?<!\d)((?:\+91[\-\s]?|91[\-\s]?|0)?[6-9]\d{9})(?!\d)'


def flatten_json(obj, prefix=""):
    flat = {}
    if isinstance(obj, dict):
        for k, v in obj.items():
            path = f"{prefix}{k}"
            if isinstance(v, (dict, list)):
                flat.update(flatten_json(v, path + "."))
            else:
                flat[path] = v
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            flat.update(flatten_json(item, f"{prefix}[{i}]."))
    return flat


def load_records():
    """
    Read INPUT_FILE line-by-line, skip blank lines,
    detect 'Set Name:' headers, buffer JSON until braces balance.
    Returns:
      records: list of (set_name, key, raw_json)
      warnings: count of skipped/malformed
    """
    records = []
    warnings = 0

    header_re = re.compile(r"^Set\s+Name:\s*([^,]+),\s*Key:\s*(.+)", re.IGNORECASE)

    buffer = []
    in_record = False
    brace_count = 0
    set_name = key = None

    with open(INPUT_FILE, "r", encoding="utf-8", errors="ignore") as f:
        for raw in f:
            line = raw.rstrip("\r\n")
            if not line.strip():
                continue

            if line.startswith("Set Name:"):
                if in_record and brace_count != 0:
                    warnings += 1
                buffer = [line]
                m = header_re.match(line)
                if m:
                    set_name = m.group(1).strip()
                    key      = m.group(2).strip().rstrip(",")
                else:
                    set_name = key = None
                brace_count = line.count("{") - line.count("}")
                in_record = True
                continue

            if in_record:
                buffer.append(line)
                brace_count += line.count("{") - line.count("}")
                if brace_count == 0:
                    header = buffer[0]
                    if "JSON Data:" not in header or not header_re.match(header):
                        warnings += 1
                    else:
                        # extract JSON
                        _, json_start = header.split("JSON Data:", 1)
                        raw_json = "\n".join([json_start] + buffer[1:])
                        records.append((set_name, key, raw_json))
                    in_record = False
                continue

            # non-blank outside record header
            warnings += 1

        if in_record and brace_count != 0:
            warnings += 1

    return records, warnings


def chunk_records(records):
    for i in range(0, len(records), CHUNK_SIZE):
        yield records[i : i + CHUNK_SIZE], i // CHUNK_SIZE


def process_chunk(args):
    chunk, idx, result_list = args
    pat = re.compile(MOBILE_REGEX)

    doc = Document()
    rows = []
    recs_seen = recs_with = parse_fail = 0
    parse_fail_details = []

    for set_name, key, raw_json in chunk:
        recs_seen += 1
        try:
            obj = json.loads(raw_json)
            flat = flatten_json(obj)
        except json.JSONDecodeError:
            parse_fail += 1
            parse_fail_details.append((set_name, key, raw_json))
            continue

        values = [
            json.dumps(v) if isinstance(v, (dict, list)) else str(v)
            for v in flat.values()
        ]

        for m in pat.finditer(raw_json):
            mobile = m.group(1)
            if not any(mobile in v for v in values):
                continue
            recs_with += 1

            # Add to Word
            para = doc.add_paragraph(f"{set_name} | {key} | ")
            para.add_run(raw_json.replace("\n", " "))
            run = para.add_run(f" | {mobile}")
            run.font.color.rgb = RGBColor(255, 0, 0)

            # Find field path
            field = ""
            for path, v in flat.items():
                txt = json.dumps(v) if isinstance(v, (dict, list)) else str(v)
                if mobile in txt:
                    field = path
                    break
            run2 = para.add_run(f" | field: {field}")
            run2.font.color.rgb = RGBColor(255, 0, 0)

            rows.append((set_name, key, raw_json, mobile, field))

    if rows:
        os.makedirs(TEMP_DIR, exist_ok=True)
        doc.save(os.path.join(TEMP_DIR, f"chunk_{idx}.docx"))

    result_list.append((rows, recs_seen, recs_with, parse_fail, parse_fail_details))


def merge_word():
    merged = Document()
    for fn in tqdm(sorted(os.listdir(TEMP_DIR)), desc="Merging Word"):
        if not fn.endswith(".docx"):
            continue
        sub = Document(os.path.join(TEMP_DIR, fn))
        for para in sub.paragraphs:
            out = merged.add_paragraph()
            for r in para.runs:
                nr = out.add_run(r.text)
                if r.font.color and r.font.color.rgb:
                    nr.font.color.rgb = r.font.color.rgb
                nr.bold, nr.italic, nr.underline = r.bold, r.italic, r.underline
    merged.save(OUTPUT_DOCX)


def write_excel(rows):
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    ws = wb.add_worksheet()
    red = wb.add_format({"font_color": "red"})
    ws.write_row(0, 0, ["Set Name", "Key", "Full JSON", "Mobile", "Field"])
    r = 1
    for a, b, c, m, f in rows:
        ws.write(r, 0, a)
        ws.write(r, 1, b)
        ws.write(r, 2, c)
        ws.write(r, 3, m, red)
        ws.write(r, 4, f)
        r += 1
    wb.close()


if __name__ == "__main__":
    # Load
    print("Loading Aerospike records …")
    records, warn_count = load_records()
    print(f"→ Total records loaded: {len(records)}  (warnings: {warn_count})")

    # Process
    mgr = Manager()
    results = mgr.list()
    chunks = list(chunk_records(records))
    with Pool(min(len(chunks), cpu_count())) as pool:
        list(
            tqdm(
                pool.imap_unordered(
                    process_chunk,
                    [(chunk, idx, results) for chunk, idx in chunks]
                ),
                total=len(chunks),
                desc="Processing records",
            )
        )

    # Aggregate
    all_rows = []
    total_seen = total_found = total_parse_fail = 0
    parse_fail_details = []
    for rows, seen, found, pfail, pfd in results:
        all_rows.extend(rows)
        total_seen += seen
        total_found += found
        total_parse_fail += pfail
        parse_fail_details.extend(pfd)

    # Summary
    print(
        f"\nScanned {total_seen} records, "
        f"{total_found} had matches, "
        f"{len(all_rows)} total hits, "
        f"{total_parse_fail} JSON parse failures"
    )
    if parse_fail_details:
        print("\nJSON parse failures:")
        for sn, k, raw in parse_fail_details:
            print(f"  {sn} | {k} → {raw}\n")

    # Output
    if os.path.isdir(TEMP_DIR) and total_found:
        merge_word()
    write_excel(all_rows)
    if os.path.isdir(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)

    print(f"\n→ Word saved: {OUTPUT_DOCX}")
    print(f"→ Excel saved: {OUTPUT_XLSX}\n")
