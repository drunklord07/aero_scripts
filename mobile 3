import os
import re
import json
import shutil
from multiprocessing import Pool, Manager, cpu_count
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE   = "input.txt"                # your Aerospike log file
CHUNK_SIZE   = 2000                       # how many records to hand a worker at once
OUTPUT_DOCX  = "aerospike_mobiles.docx"
OUTPUT_XLSX  = "aerospike_mobiles.xlsx"
TEMP_DIR     = "temp_aero_parts"

MOBILE_REGEX = r'(?<!\d)((?:\+91[\-\s]?|91[\-\s]?|0)?[6-9]\d{9})(?!\d)'

# === FLATTEN JSON ===
def flatten_json(obj, prefix=""):
    flat = {}
    if isinstance(obj, dict):
        for k, v in obj.items():
            path = f"{prefix}{k}"
            if isinstance(v, (dict, list)):
                flat.update(flatten_json(v, path + "."))
            elif isinstance(v, str) and v.strip().startswith(("{", "[")):
                try:
                    inner = json.loads(v)
                except:
                    flat[path] = v
                else:
                    flat.update(flatten_json(inner, path + "."))
            else:
                flat[path] = v
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            flat.update(flatten_json(item, f"{prefix}[{i}]."))
    return flat

# === LOAD & GROUP RECORDS ===
def load_records():
    """
    Read INPUT_FILE line-by-line, buffer JSON under 'Set Name:' headers,
    skip pure-blank lines, ignore any bytearray(...) in the header when
    matching/counting braces, and return:
      - records: [(set_name, key, raw_json), ...]
      - load_warnings: total skipped/malformed
      - skipped_lines: [(lineno, text), ...] for non-blank skips
    """
    records = []
    load_warnings = 0
    skipped_lines = []

    buffer = []
    in_record = False
    brace_count = 0
    header_re = re.compile(r"^Set\s+Name:\s*([^,]+),\s*Key:\s*(.+)", re.IGNORECASE)

    with open(INPUT_FILE, "r", encoding="utf-8", errors="ignore") as f:
        for lineno, raw in enumerate(f, start=1):
            line = raw.rstrip("\r\n")
            # skip blank
            if not line.strip():
                continue

            if line.startswith("Set Name:"):
                # drop any previous unclosed record
                if in_record and brace_count != 0:
                    load_warnings += 1
                    skipped_lines.append((lineno, buffer[0]))
                buffer = [line]

                # strip out bytearray(...) entirely before counting
                clean_hdr = re.sub(r"bytearray\([^)]*\)", "", line)
                if "JSON Data:" in clean_hdr:
                    # count only in the JSON part
                    _, json_part = clean_hdr.split("JSON Data:", 1)
                    brace_count = json_part.count("{") - json_part.count("}")
                else:
                    brace_count = 0
                in_record = True

            elif in_record:
                buffer.append(line)
                brace_count += line.count("{") - line.count("}")

                if brace_count == 0:
                    header_line = buffer[0]
                    clean_hdr = re.sub(r"bytearray\([^)]*\)", "", header_line)
                    if "JSON Data:" not in clean_hdr:
                        load_warnings += 1
                        skipped_lines.append((lineno, header_line))
                    else:
                        hdr, json_start = clean_hdr.split("JSON Data:", 1)
                        m = header_re.match(hdr)
                        if not m:
                            load_warnings += 1
                            skipped_lines.append((lineno, header_line))
                        else:
                            set_name = m.group(1).strip()
                            key = m.group(2).strip().rstrip(",")
                            raw_json = "\n".join([json_start] + buffer[1:])
                            records.append((set_name, key, raw_json))
                    in_record = False

            else:
                # non-blank outside record
                load_warnings += 1
                skipped_lines.append((lineno, line))

        # leftover open record
        if in_record and brace_count != 0:
            load_warnings += 1
            skipped_lines.append(("EOF", buffer[0] if buffer else ""))

    return records, load_warnings, skipped_lines

# === CHUNKING ===
def chunk_records(records):
    for i in range(0, len(records), CHUNK_SIZE):
        yield records[i : i + CHUNK_SIZE], i // CHUNK_SIZE

# === PROCESS ONE CHUNK ===
def process_chunk(args):
    chunk, idx, result_list = args
    pat = re.compile(MOBILE_REGEX)

    doc = Document()
    rows = []
    recs_seen = recs_with = parse_fail = 0
    parse_fail_details = []

    for set_name, key, raw in chunk:
        recs_seen += 1
        try:
            obj = json.loads(raw)
            flat = flatten_json(obj)
        except json.JSONDecodeError:
            parse_fail += 1
            snippet = raw[:100].replace("\n", " ") + "â€¦"
            parse_fail_details.append((set_name, key, snippet))
            continue

        vals = [json.dumps(v) if isinstance(v, (dict, list)) else str(v)
                for v in flat.values()]

        raw_unesc = raw.replace('\\"', '"').replace("\\{", "{").replace("\\}", "}")
        hits = list(pat.finditer(raw_unesc))
        true_hits = [(m.group(1), m.span(1)) for m in hits
                     if any(m.group(1) in v for v in vals)]
        if not true_hits:
            continue

        recs_with += 1
        para = doc.add_paragraph(f"{set_name} | {key} | ")
        last = 0
        fields = []

        for mobile, (s, e) in sorted(true_hits, key=lambda x: x[1][0]):
            if s > last:
                para.add_run(raw_unesc[last:s])
            run = para.add_run(mobile)
            run.font.color.rgb = RGBColor(255, 0, 0)
            last = e

            fld = ""
            for path, v in flat.items():
                text = json.dumps(v) if isinstance(v, (dict, list)) else str(v)
