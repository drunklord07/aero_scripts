import os
import re
import json
import shutil
from multiprocessing import Pool, Manager, cpu_count
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE   = "input.txt"
CHUNK_SIZE   = 2000
OUTPUT_DOCX  = "aerospike_mobiles.docx"
OUTPUT_XLSX  = "aerospike_mobiles.xlsx"
TEMP_DIR     = "temp_aero_parts"

MOBILE_REGEX = r'(?<!\d)((?:\+91[\-\s]?|91[\-\s]?|0)?[6-9]\d{9})(?!\d)'

# === FLATTEN JSON ===
def flatten_json(obj, prefix=""):
    flat = {}
    if isinstance(obj, dict):
        for k, v in obj.items():
            path = f"{prefix}{k}"
            if isinstance(v, (dict, list)):
                flat.update(flatten_json(v, path + "."))
            elif isinstance(v, str) and v.strip().startswith(("{", "[")):
                try:
                    inner = json.loads(v)
                except:
                    flat[path] = v
                else:
                    flat.update(flatten_json(inner, path + "."))
            else:
                flat[path] = v
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            flat.update(flatten_json(item, f"{prefix}[{i}]."))
    return flat

# === LOAD RECORDS ===
def load_records():
    """
    Read INPUT_FILE, detect every 'Set Name:' header, and
    always produce one record per header. If JSON Data: is missing
    or braces never close, raw_json is set to "" (no parse attempt).
    Returns:
      records: [(set_name, key, raw_json), ...]
      warnings: count of malformed headers
      skipped: list of (lineno, header_line) for headers malformed or missing Set Name
    """
    header_re = re.compile(
        r"Set\s+Name\s*:\s*([^,]+)"
        r"(?:\s*,\s*Key\s*:\s*([^,]+))?",
        flags=re.IGNORECASE
    )

    records = []
    warnings = 0
    skipped = []

    lines = open(INPUT_FILE, encoding="utf-8", errors="ignore").read().splitlines()
    n = len(lines)
    i = 0

    while i < n:
        line = lines[i].rstrip()
        if not line.strip():
            i += 1
            continue
        if line.lower().startswith("set name:"):
            m = header_re.match(line)
            if not m:
                warnings += 1
                skipped.append((i+1, line))
                i += 1
                continue

            set_name = m.group(1).strip()
            key = (m.group(2) or "").strip()
            raw_json = ""

            # find JSON Data: within this header or subsequent lines
            j = line.find("JSON Data:")
            if j >= 0:
                # header contains JSON Data
                json_part = line[j+len("JSON Data:"):]
                brace_count = json_part.count("{") - json_part.count("}")
                buffer = [json_part]
                i += 1
                # buffer until braces balance or EOF
                while i < n and brace_count > 0:
                    l2 = lines[i]
                    buffer.append(l2)
                    brace_count += l2.count("{") - l2.count("}")
                    i += 1
                if brace_count == 0:
                    raw_json = "\n".join(buffer)
                else:
                    warnings += 1
                    skipped.append((i, f"unclosed JSON for {set_name}"))
            else:
                # no JSON Data marker: warn but still record empty raw_json
                warnings += 1
                skipped.append((i+1, f"No JSON Data: in header for {set_name}"))
                i += 1

            records.append((set_name, key, raw_json))
        else:
            i += 1

    return records, warnings, skipped

# === CHUNK RECORDS ===
def chunk_records(records):
    for idx in range(0, len(records), CHUNK_SIZE):
        yield records[idx:idx+CHUNK_SIZE], idx//CHUNK_SIZE

# === PROCESS CHUNK ===
def process_chunk(args):
    chunk, idx, result_list = args
    pat = re.compile(MOBILE_REGEX)

    doc = Document()
    rows = []
    seen = with_mob = parse_fail = 0
    parse_fail_details = []

    for set_name, key, raw in chunk:
        seen += 1
        if not raw:
            continue
        try:
            obj = json.loads(raw)
            flat = flatten_json(obj)
        except:
            parse_fail += 1
            snippet = raw[:100].replace("\n"," ") + "…"
            parse_fail_details.append((set_name, key, snippet))
            continue

        vals = [json.dumps(v) if isinstance(v,(dict,list)) else str(v)
                for v in flat.values()]
        hits = [(m.group(1), m.span(1))
                for m in pat.finditer(raw)
                if any(m.group(1) in v for v in vals)]
        if not hits:
            continue

        with_mob += 1
        para = doc.add_paragraph(f"{set_name} | {key} | ")
        last = 0
        fields = []
        for mob, (s,e) in sorted(hits, key=lambda x:x[1][0]):
            if s>last: para.add_run(raw[last:s])
            run = para.add_run(mob); run.font.color.rgb = RGBColor(255,0,0)
            last = e
            fld=""
            for path,v in flat.items():
                txt = json.dumps(v) if isinstance(v,(dict,list)) else str(v)
                if mob in txt:
                    fld=path; break
            fields.append(fld)
        if last < len(raw): para.add_run(raw[last:])
        para.add_run(" | field: ")
        for i,fld in enumerate(fields):
            if i: para.add_run(", ")
            fr = para.add_run(fld); fr.font.color.rgb=RGBColor(255,0,0)
        for mob,_ in hits:
            rows.append((set_name, key, raw, mob, fields.pop(0)))

    if rows:
        os.makedirs(TEMP_DIR, exist_ok=True)
        doc.save(os.path.join(TEMP_DIR, f"chunk_{idx}.docx"))

    result_list.append((rows, seen, with_mob, parse_fail, parse_fail_details))

# === MERGE .docx ===
def merge_word():
    merged = Document()
    for fn in tqdm(sorted(os.listdir(TEMP_DIR)), desc="Merging Word"):
        if not fn.endswith(".docx"): continue
        for p in Document(os.path.join(TEMP_DIR,fn)).paragraphs:
            np = merged.add_paragraph()
            for r in p.runs:
                nr=np.add_run(r.text)
                if r.font.color and r.font.color.rgb:
                    nr.font.color.rgb=r.font.color.rgb
                nr.bold,nr.italic,nr.underline = r.bold,r.italic,r.underline
    merged.save(OUTPUT_DOCX)

# === WRITE EXCEL ===
def write_excel(rows):
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    ws = wb.add_worksheet()
    red = wb.add_format({"font_color":"red"})
    ws.write_row(0,0,["Set Name","Key","Full JSON","Mobile","Field"])
    r=1
    for a,b,c,m,f in rows:
        ws.write(r,0,a); ws.write(r,1,b); ws.write(r,2,c)
        ws.write(r,3,m,red); ws.write(r,4,f)
        r+=1
    wb.close()

if __name__=="__main__":
    if os.path.isdir(TEMP_DIR): shutil.rmtree(TEMP_DIR)
    print("Loading records…")
    recs, warns, skipped = load_records()
    print(f"Loaded {len(recs)} records, warnings: {warns}")
    if skipped:
        print("Skipped headers:")
        for ln,hd in skipped: print(" ",ln,hd)
    mgr=Manager(); results=mgr.list()
    ch=list(chunk_records(recs))
    with Pool(min(len(ch),cpu_count())) as p:
        list(tqdm(p.imap_unordered(process_chunk,
             [(c,i,results) for c,i in ch]),
             total=len(ch),desc="Processing"))
    all_rows=[];tot=wm=pf=0;pfails=[]
    for rows,seen,wmc,pf,pfd in results:
        all_rows+=rows; tot+=seen; wm+=wmc; pf+=pf; pfails+=pfd
    print(f"Scanned {tot}, {wm} with mobiles, {len(all_rows)} hits, {pf} JSON errors")
    if pfails:
        print("JSON errors:")
        for s,k,sp in pfails: print(" ",s,k,sp)
    if os.path.isdir(TEMP_DIR) and tot>0: merge_word()
    write_excel(all_rows)
    shutil.rmtree(TEMP_DIR,ignore_errors=True)
    print("Done →", OUTPUT_DOCX, OUTPUT_XLSX)```
