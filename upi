import os
import re
import json
import shutil
import sys
from multiprocessing import Pool, Manager, cpu_count
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE   = "input.txt"
CHUNK_SIZE   = 2000
OUTPUT_DOCX  = "aerospike_upi.docx"
OUTPUT_XLSX  = "aerospike_upi.xlsx"
TEMP_DIR     = "temp_aero_upi_parts"
UPI_REGEX    = r'(?<!\w)([A-Za-z0-9.\-_]{2,256}@[A-Za-z]{2,64})(?![\w\.])'

# === FLATTEN JSON ===
def flatten_json(obj, prefix=""):
    flat = {}
    if isinstance(obj, dict):
        for k, v in obj.items():
            path = f"{prefix}{k}"
            if isinstance(v, (dict, list)):
                flat.update(flatten_json(v, path + "."))
            else:
                flat[path] = v
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            flat.update(flatten_json(item, f"{prefix}[{i}]."))
    return flat

# === LOAD & GROUP RECORDS ===
def load_records():
    """
    Single-pass header + brace-scanning loader.
    Returns:
      records: list of (set_name, key, raw_json)
      warnings: count of malformed or unbalanced blocks
      skipped: list of (set_name, key, reason)
    """
    text = open(INPUT_FILE, encoding="utf-8", errors="ignore").read()
    header_pat = re.compile(
        r"Set\s+Name\s*:\s*(?P<set>[^,]+)"
        r"(?:\s*,\s*Key\s*:\s*(?P<key>[^,]+))?"
        r".*?JSON\s*Data\s*:",
        flags=re.IGNORECASE
    )

    records, skipped, warnings = [], [], 0
    pos, L = 0, len(text)

    while True:
        m = header_pat.search(text, pos)
        if not m:
            break
        set_name = m.group("set").strip()
        key      = (m.group("key") or "").strip()
        brace_i  = text.find("{", m.end())
        if brace_i < 0:
            warnings += 1
            skipped.append((set_name, key, "no JSON opening brace"))
            pos = m.end()
            continue
        count, i = 1, brace_i + 1
        while i < L and count:
            if text[i] == "{":
                count += 1
            elif text[i] == "}":
                count -= 1
            i += 1
        if count != 0:
            warnings += 1
            skipped.append((set_name, key, "unbalanced JSON braces"))
            pos = m.end()
            continue
        raw_json = text[brace_i:i]
        records.append((set_name, key, raw_json))
        pos = i

    return records, warnings, skipped

# === SPLIT INTO CHUNKS ===
def chunk_records(records):
    for idx in range(0, len(records), CHUNK_SIZE):
        yield records[idx:idx+CHUNK_SIZE], idx // CHUNK_SIZE

# === PROCESS ONE CHUNK (UPI EXTRACTION) ===
def process_chunk(args):
    chunk, idx, result_list = args
    pat = re.compile(UPI_REGEX)

    doc = Document()
    rows = []
    seen = had_upi = hits = parse_fail = 0
    parse_errors = []

    for set_name, key, raw_json in chunk:
        seen += 1
        try:
            obj = json.loads(raw_json)
            flat = flatten_json(obj)
        except json.JSONDecodeError:
            parse_fail += 1
            parse_errors.append((set_name, key, raw_json))
            continue

        values = [
            json.dumps(v) if isinstance(v, (dict, list)) else str(v)
            for v in flat.values()
        ]

        this_hits = []
        for m in pat.finditer(raw_json):
            upi = m.group(1)
            if any(upi in v for v in values):
                this_hits.append((upi, m.span(1)))
        if not this_hits:
            continue

        had_upi += 1
        para = doc.add_paragraph(f"{set_name} | {key} | ")
        para.add_run(raw_json.replace("\n", " "))

        for upi, _ in sorted(this_hits, key=lambda x: x[1][0]):
            hits += 1
            run1 = para.add_run(f" | {upi}")
            run1.font.color.rgb = RGBColor(255, 0, 0)
            # find field path
            field = ""
            for path, v in flat.items():
                txt = json.dumps(v) if isinstance(v, (dict, list)) else str(v)
                if upi in txt:
                    field = path
                    break
            run2 = para.add_run(f" | field: {field}")
            run2.font.color.rgb = RGBColor(255, 0, 0)
            rows.append((set_name, key, raw_json, upi, field))

    if rows:
        os.makedirs(TEMP_DIR, exist_ok=True)
        doc.save(os.path.join(TEMP_DIR, f"chunk_{idx}.docx"))

    result_list.append((rows, seen, had_upi, hits, parse_fail, parse_errors))

# === MERGE DOCX ===
def merge_word():
    merged = Document()
    for fn in tqdm(sorted(os.listdir(TEMP_DIR)), desc="Merging Word"):
        if not fn.endswith(".docx"):
            continue
        sub = Document(os.path.join(TEMP_DIR, fn))
        for para in sub.paragraphs:
            out = merged.add_paragraph()
            for run in para.runs:
                nr = out.add_run(run.text)
                if run.font.color and run.font.color.rgb:
                    nr.font.color.rgb = run.font.color.rgb
                nr.bold, nr.italic, nr.underline = run.bold, run.italic, run.underline
    merged.save(OUTPUT_DOCX)

# === WRITE EXCEL (Matches + inline Parse Failures) ===
def write_excel(rows, parse_errors):
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    ws = wb.add_worksheet("Results")
    red = wb.add_format({"font_color": "red"})

    # Write matches
    ws.write_row(0, 0, ["Set Name","Key","Full JSON","UPI","Field"])
    r = 1
    for a, b, c, u, f in rows:
        ws.write(r, 0, a)
        ws.write(r, 1, b)
        ws.write(r, 2, c)
        ws.write(r, 3, u, red)
        ws.write(r, 4, f)
        r += 1

    # Leave two blank lines
    r += 1

    # Parse failures section
    ws.write_row(r, 0, ["Parse Failures:"])
    r += 1
    ws.write_row(r, 0, ["Set Name","Key","Raw JSON"])
    r += 1
    for a, b, raw in parse_errors:
        ws.write(r, 0, a)
        ws.write(r, 1, b)
        ws.write(r, 2, raw)
        r += 1

    wb.close()

if __name__ == "__main__":
    # 1) Load
    print("Loading Aerospike records …")
    records, warn_count, skipped = load_records()
    print(f"→ Total records loaded: {len(records)}  (warnings: {warn_count})")

    # 2) Process chunks
    mgr = Manager()
    results = mgr.list()
    chunks = list(chunk_records(records))
    with Pool(min(len(chunks), cpu_count())) as pool:
        list(tqdm(
            pool.imap_unordered(
                process_chunk,
                [(chunk, idx, results) for chunk, idx in chunks]
            ),
            total=len(chunks),
            desc="Processing records",
        ))

    # 3) Aggregate
    all_rows = []
    total_seen = total_with_upi = total_hits = total_pf = 0
    parse_errors = []
    for rows, seen, with_upi, hits, pfail, pfd in results:
        all_rows.extend(rows)
        total_seen += seen
        total_with_upi += with_upi
        total_hits += hits
        total_pf += pfail
        parse_errors.extend(pfd)

    # 4) Summary
    print(
        f"\nScanned {total_seen} records, "
        f"{total_with_upi} had UPIs, "
        f"{total_hits} total UPIs, "
        f"{total_pf} JSON parse failures"
    )

    # 5) Output
    if os.path.isdir(TEMP_DIR) and total_with_upi > 0:
        merge_word()
    write_excel(all_rows, parse_errors)
    if os.path.isdir(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)

    print(f"\n→ Word saved: {OUTPUT_DOCX}")
    print(f"→ Excel saved: {OUTPUT_XLSX}\n")
    sys.exit(0 if total_hits > 0 and total_pf == 0 else 1)
