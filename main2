#!/usr/bin/env python3
import os
import glob
import re
import subprocess
import argparse
import shutil
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import pandas as pd

# ASCII-safe print
def safe_print(*args, **kwargs):
    msg = " ".join(str(a) for a in args)
    msg = msg.encode("ascii", "ignore").decode("ascii")
    print(msg, **kwargs)

# Ensure child scripts print UTF-8 safely
ENV = os.environ.copy()
ENV["PYTHONIOENCODING"] = "utf-8"

# For grabbing summary & hits
SUMMARY_RE     = re.compile(r"records.*?hits", re.IGNORECASE)
HITS_RE        = re.compile(r",\s*(\d+)\s+.*?hits", re.IGNORECASE)
PARSE_FAIL_RE  = re.compile(r"JSON parse failures?:", re.IGNORECASE)

def find_scripts():
    me = os.path.abspath(__file__)
    return sorted(
        fn for fn in glob.glob("**/*.py", recursive=True)
        if os.path.abspath(fn) != me
    )

def parse_expected_xlsx(path):
    base = os.path.splitext(os.path.basename(path))[0]
    text = open(path, encoding="utf-8", errors="ignore").read()
    m = re.search(r'OUTPUT_XLSX\s*=\s*["\'](.+?\.xlsx)["\']', text)
    return m.group(1) if m else f"{base}.xlsx"

def run_script(path):
    xlsx = parse_expected_xlsx(path)
    proc = subprocess.run(
        [sys.executable, path],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        env=ENV,
        errors="replace"
    )
    log = proc.stdout

    # find summary line
    summary = "[no summary found]"
    for ln in log.splitlines():
        if SUMMARY_RE.search(ln):
            summary = ln.strip()
            break

    # extract hits
    mh = HITS_RE.search(summary)
    hits = int(mh.group(1)) if mh else 0

    # collect parse failure lines
    pf_lines = []
    in_pf = False
    for ln in log.splitlines():
        if PARSE_FAIL_RE.search(ln):
            in_pf = True
            pf_lines.append(ln.strip())
        elif in_pf:
            if ln.strip().startswith("-") or ln.strip():
                pf_lines.append(ln.strip())
            else:
                break

    # did .xlsx appear?
    exists = os.path.isfile(xlsx)
    return {
        "basename":      os.path.splitext(os.path.basename(path))[0],
        "xlsx":          xlsx,
        "exists":        exists,
        "hits":          hits,
        "summary":       summary,
        "parse_failures":pf_lines,
        "log":           log
    }

def main():
    base = os.path.dirname(os.path.abspath(__file__))
    os.chdir(base)

    parser = argparse.ArgumentParser()
    parser.add_argument("-c","--combined",default="combined_output.xlsx")
    parser.add_argument("-j","--max-parallel",type=int,default=3)
    args = parser.parse_args()

    scripts = find_scripts()
    safe_print(f"Found {len(scripts)} scripts.\n")

    out_dir = os.path.join(base, "outputs")
    if os.path.isdir(out_dir):
        shutil.rmtree(out_dir)
    os.makedirs(out_dir)

    results = []
    total_hits = 0

    with ThreadPoolExecutor(max_workers=args.max_parallel) as exe:
        futures = {exe.submit(run_script, s): s for s in scripts}
        for fut in tqdm(as_completed(futures),
                        total=len(futures),
                        desc="Running scripts",
                        ascii=True):
            res = fut.result()
            results.append(res)
            total_hits += res["hits"]
            name = res["basename"]

            # Only check whether xlsx exists
            if not res["exists"]:
                safe_print(f"[WARN]  {name} missing {res['xlsx']}")
            else:
                safe_print(f"[OK]    {name}")

            # print summary
            safe_print("  " + res["summary"])
            # print JSON parse failures
            for pf in res["parse_failures"]:
                safe_print("   " + pf)

            # move outputs
            if res["exists"]:
                # move xlsx
                shutil.move(res["xlsx"], os.path.join(out_dir, res["xlsx"]))
                # move docx if present
                docx = res["xlsx"][:-5] + ".docx"
                if os.path.isfile(docx):
                    shutil.move(docx, os.path.join(out_dir, os.path.basename(docx)))

    total = len(results)
    ok = sum(1 for r in results if r["exists"])
    safe_print(f"\nScripts run: {total}, produced XLSX: {ok}, missing: {total-ok}")
    safe_print(f"Grand total hits: {total_hits}\n")

    excels = sorted(f for f in os.listdir(out_dir) if f.endswith(".xlsx"))
    if not excels:
        safe_print("No Excel files to merge.")
        sys.exit(1)

    safe_print(f"Merging {len(excels)} Excel files into '{args.combined}' â€¦")
    with pd.ExcelWriter(args.combined, engine="xlsxwriter") as writer:
        for fname in tqdm(excels, desc="Merging", ascii=True):
            path = os.path.join(out_dir, fname)
            sheet = os.path.splitext(fname)[0][:31]
            try:
                df = pd.read_excel(path, dtype=str)
                df.to_excel(writer, sheet_name=sheet, index=False)
            except Exception:
                writer.book.add_worksheet(sheet)

    safe_print(f"\nMaster workbook created: {os.path.abspath(args.combined)}")
    safe_print(f"All outputs in: {os.path.abspath(out_dir)}")
    sys.exit(0 if ok==total else 1)

if __name__=="__main__":
    main()
