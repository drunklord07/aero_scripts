import os
import re
import json
import shutil
from multiprocessing import Pool, Manager, cpu_count
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE   = "input.txt"                  # your Aerospike log file
CHUNK_SIZE   = 2000                         # how many records to hand a worker at once
OUTPUT_DOCX  = "aerospike_mac.docx"
OUTPUT_XLSX  = "aerospike_mac.xlsx"
TEMP_DIR     = "temp_aero_mac_parts"

# Regex for MAC addresses: six pairs of hex digits separated by :
# or -, e.g. "01:23:45:67:89:AB" or "01-23-45-67-89-AB"
MAC_REGEX    = r'(?:[0-9A-Fa-f]{2}[:-]){5}(?:[0-9A-Fa-f]{2})'
MAC_BODY     = r'(?:[0-9A-Fa-f]{2}[:-]){5}(?:[0-9A-Fa-f]{2})'

# Exact-match JSON fallback for MAC (similar to KV_EXACT_RE logic)
MAC_EXACT_RE = re.compile(
    rf'"([^"]+)"\s*:\s*"\[?({MAC_BODY})\]?"(?:\.)?|'           # "key":"[MAC]" or "key":"MAC" + optional dot
    rf'"([^"]+)"\s*:\s*\[?({MAC_BODY})\]?(?=[\s,}}\]\.]|$)'    # "key":[MAC] or "key":MAC + lookahead
)

# XML fallback: <… name="mac" … value="01:23:45:67:89:AB" …>
XML_NAMED_RE = re.compile(
    rf'<[^>]*\bname\s*=\s*"mac"[^>]*\bvalue\s*=\s*"({MAC_BODY})"[^>]*>',
    flags=re.IGNORECASE
)


# === UTILITY: FLATTEN JSON ===
def flatten_json(obj, prefix=""):
    """
    Recursively flatten a nested JSON object (dicts and lists).
    Produces a dict { "parent.child": value, ... }.
    If a value is a string that itself looks like JSON (starts with { or [),
    attempt to parse and flatten that too.
    """
    flat = {}
    if isinstance(obj, dict):
        for k, v in obj.items():
            path = f"{prefix}{k}"
            if isinstance(v, (dict, list)):
                flat.update(flatten_json(v, path + "."))
            elif isinstance(v, str) and v.strip().startswith(("{", "[")):
                try:
                    inner = json.loads(v)
                except:
                    flat[path] = v
                else:
                    flat.update(flatten_json(inner, path + "."))
            else:
                flat[path] = v
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            flat.update(flatten_json(item, f"{prefix}[{i}]."))
    return flat


# === LOAD & GROUP RECORDS ===
def load_records():
    """
    Read INPUT_FILE line by line. Whenever a line starts with "Set Name:",
    begin buffering until braces { } balance. Extract:
      - set_name (string)
      - key (string)
      - raw_json (string including braces)
    If "JSON Data:" is missing in the header or braces never balance,
    increment load_warnings and skip that record.
    Returns:
      records = [ (set_name, key, raw_json), … ]
      load_warnings = total number of skipped/malformed records
    """
    records = []
    load_warnings = 0

    buffer = []
    in_record = False
    brace_count = 0
    header_re = re.compile(r"^Set\s+Name:\s*([^,]+),\s*Key:\s*(.+)", re.IGNORECASE)

    with open(INPUT_FILE, "r", encoding="utf-8", errors="ignore") as f:
        for raw_line in f:
            line = raw_line.rstrip("\r\n")
            # Start of a new record
            if line.startswith("Set Name:"):
                if in_record and brace_count != 0:
                    load_warnings += 1
                buffer = [line]
                brace_count = line.count("{") - line.count("}")
                in_record = True

            elif in_record:
                buffer.append(line)
                brace_count += line.count("{") - line.count("}")

                # If braces balance, finalize the record
                if brace_count == 0:
                    # Must have "JSON Data:" in the header
                    if "JSON Data:" not in buffer[0]:
                        load_warnings += 1
                        in_record = False
                        continue

                    header_part, json_start = buffer[0].split("JSON Data:", 1)
                    m = header_re.match(header_part)
                    if not m:
                        load_warnings += 1
                        in_record = False
                        continue

                    set_name = m.group(1).strip()
                    key = m.group(2).strip().rstrip(",")
                    json_lines = [json_start] + buffer[1:]
                    raw_json = "\n".join(json_lines)

                    records.append((set_name, key, raw_json))
                    in_record = False

            else:
                # Any line outside a valid record header is a warning
                load_warnings += 1

        # If file ends with an unclosed record
        if in_record and brace_count != 0:
            load_warnings += 1

    return records, load_warnings


# === SPLIT INTO CHUNKS ===
def chunk_records(records):
    """
    Yield (records_chunk, chunk_index) in slices of CHUNK_SIZE.
    """
    for i in range(0, len(records), CHUNK_SIZE):
        yield records[i : i + CHUNK_SIZE], (i // CHUNK_SIZE)


# === PROCESS ONE CHUNK (MAC EXTRACTION) ===
def process_chunk(args):
    """
    Each chunk is a list of (set_name, key, raw_json).
    For each record:
      1. Try json.loads(raw_json). If it fails, increment parse_failures.
      2. Flatten the JSON into { path: value, ... }.
      3. Build a list of all flattened values as strings.
      4. Run MAC_REGEX on the raw JSON text (unescaped), collect hits.
      5. For each hit, check it appears in one of the flattened values;
         only keep those “true_hits.”
      6. If any true_hits exist, increment recs_with_mac,
         create a Word paragraph that:
           - Copies raw JSON up to each match,
           - Inserts the matched MAC in red,
           - Continues, then appends “| field: <path1>, <path2>, …”
             (each field path also in red).
         And add one tuple (set_name, key, raw_json, mac, field) per match
         into matches_data.
      7. Save this chunk’s .docx if matches_data is nonempty.
      8. Append (matches_data, recs_seen, recs_with_mac, parse_failures)
         to result_list.
    """
    chunk, idx, result_list = args
    pat_mac = re.compile(MAC_REGEX)

    doc = Document()
    matches_data = []
    recs_seen = 0
    recs_with_mac = 0
    parse_failures = 0

    for (set_name, key, raw_json) in chunk:
        recs_seen += 1
        # 1) Parse JSON
        try:
            obj = json.loads(raw_json)
            flat = flatten_json(obj)
        except json.JSONDecodeError:
            parse_failures += 1
            continue

        # 2) Collect all flattened values as strings
        values_to_check = []
        for v in flat.values():
            if isinstance(v, (dict, list)):
                values_to_check.append(json.dumps(v))
            else:
                values_to_check.append(str(v))

        # 3) XML fallback map: if <… name="mac" value="01:23:..." …>
        xml_map = {m.group(1): "mac.value" for m in XML_NAMED_RE.finditer(raw_json)}

        # 4) Scan raw JSON (unescaped) for MACs
        raw_unesc = raw_json.replace('\\"', '"').replace("\\{", "{").replace("\\}", "}")
        hits = list(pat_mac.finditer(raw_unesc))
        if not hits:
            continue

        # 5) Filter only those hits that appear in a flattened value
        true_hits = []
        for m in hits:
            candidate = m.group(1)
            if any(candidate in val for val in values_to_check):
                true_hits.append((candidate, m.span(1)))
        if not true_hits:
            continue

        recs_with_mac += 1

        # 6) Build Word paragraph with red‐colored MAC and field name
        para = doc.add_paragraph(f"{set_name} | {key} | ")
        last_pos = 0
        true_hits_sorted = sorted(true_hits, key=lambda x: x[1][0])
        fields = []

        for mac, (s, e) in true_hits_sorted:
            # Copy text before the match
            if s > last_pos:
                para.add_run(raw_unesc[last_pos : s])

            # Insert matched MAC in red
            run = para.add_run(mac)
            run.font.color.rgb = RGBColor(255, 0, 0)
            last_pos = e

            # 7) Determine JSON field path for this MAC:
            found_field = ""
            # a) First try an exact match in flattened values
            for path, v in flat.items():
                if isinstance(v, (dict, list)):
                    if mac in json.dumps(v):
                        found_field = path
                        break
                else:
                    if mac == str(v) or mac in str(v):
                        found_field = path
                        break

            # b) If still empty, try exact key:value or key:[value]
            if not found_field:
                fm = MAC_EXACT_RE.search(raw_unesc)
                if fm:
                    found_field = fm.group(1) or fm.group(3)

            # c) If still empty but XML fallback exists
            if not found_field and mac in xml_map:
                found_field = xml_map[mac]

            fields.append(found_field)

        # Copy any trailing JSON text
        if last_pos < len(raw_unesc):
            para.add_run(raw_unesc[last_pos :])

        # Append " | field: fld1, fld2, …" with each field in red
        para.add_run(" | field: ")
        for i, fld in enumerate(fields):
            if i:
                para.add_run(", ")
            fr = para.add_run(fld)
            fr.font.color.rgb = RGBColor(255, 0, 0)

        # 8) Store data for Excel: one row per matched MAC
        for mac, _ in true_hits_sorted:
            matches_data.append((set_name, key, raw_json, mac, fields.pop(0)))

    # 9) Save chunk’s .docx if any matches
    if matches_data:
        os.makedirs(TEMP_DIR, exist_ok=True)
        doc.save(os.path.join(TEMP_DIR, f"chunk_{idx}.docx"))

    result_list.append((matches_data, recs_seen, recs_with_mac, parse_failures))


# === MERGE ALL PARTIAL .docx FILES ===
def merge_word():
    """
    After all chunks finish, open each chunk_{idx}.docx in TEMP_DIR (sorted),
    append its paragraphs into one combined Document (preserving red coloring),
    then save as OUTPUT_DOCX.
    """
    merged = Document()
    for fn in tqdm(sorted(os.listdir(TEMP_DIR)), desc="Merging Word"):
        if not fn.endswith(".docx"):
            continue
        sub = Document(os.path.join(TEMP_DIR, fn))
        for para in sub.paragraphs:
            out_p = merged.add_paragraph()
            for run in para.runs:
                nr = out_p.add_run(run.text)
                if run.font.color and run.font.color.rgb:
                    nr.font.color.rgb = run.font.color.rgb
                nr.bold, nr.italic, nr.underline = run.bold, run.italic, run.underline
    merged.save(OUTPUT_DOCX)


# === WRITE EXCEL FILE ===
def write_excel(all_matches):
    """
    Create OUTPUT_XLSX with columns:
      Set Name | Key | Full JSON | MAC | Field
    The "MAC" cell is formatted in red.
    """
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    ws = wb.add_worksheet()
    red_fmt = wb.add_format({"font_color": "red"})

    headers = ["Set Name", "Key", "Full JSON", "MAC", "Field"]
    ws.write_row(0, 0, headers)
    row = 1
    for (set_name, key, raw_json, mac, field) in all_matches:
        ws.write(row, 0, set_name)
        ws.write(row, 1, key)
        ws.write(row, 2, raw_json)
        ws.write(row, 3, mac, red_fmt)
        ws.write(row, 4, field)
        row += 1

    wb.close()


# === MAIN PROGRAM ===
if __name__ == "__main__":
    # 1) Remove existing TEMP_DIR if present
    if os.path.isdir(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)

    # 2) Load & group records
    print("Loading Aerospike records …")
    records, load_warnings = load_records()
    print(f"→ Total records loaded: {len(records)}  (warnings: {load_warnings})")

    # 3) Split into chunks
    chunks = list(chunk_records(records))

    # 4) Process chunks in parallel
    mgr = Manager()
    results = mgr.list()
    with Pool(min(cpu_count(), len(chunks))) as pool:
        list(
            tqdm(
                pool.imap_unordered(
                    process_chunk,
                    [(chunk, idx, results) for (chunk, idx) in chunks],
                ),
                total=len(chunks),
                desc="Processing chunks",
            )
        )

    # 5) Aggregate results
    all_matches = []
    total_scanned = total_with_mac = total_parse_fail = 0
    for (matches_data, recs_seen, recs_with_mac, parse_failures) in results:
        all_matches.extend(matches_data)
        total_scanned += recs_seen
        total_with_mac += recs_with_mac
        total_parse_fail += parse_failures

    print(
        f"Scanned {total_scanned} records, "
        f"{total_with_mac} had ≥1 MAC matches, "
        f"{len(all_matches)} total MAC hits, "
        f"{total_parse_fail} JSON parse failures"
    )

    # 6) Merge chunked .docx files if any
    if os.path.isdir(TEMP_DIR) and total_scanned > 0:
        merge_word()

    # 7) Write the Excel output
    write_excel(all_matches)

    # 8) Clean up
    if os.path.isdir(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)

    print(f"\n→ Word  saved to {OUTPUT_DOCX}")
    print(f"→ Excel saved to {OUTPUT_XLSX}\n")
