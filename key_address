import os
import re
import json
import shutil
import sys
from multiprocessing import Pool, Manager, cpu_count
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE    = "input.txt"
CHUNK_SIZE    = 2000
OUTPUT_DOCX   = "aerospike_address.docx"
OUTPUT_XLSX   = "aerospike_address.xlsx"
TEMP_DIR      = "temp_aero_address_parts"
address_keys  = [
    "address", "full address", "complete address", "residential address", "permanent address",
    "current address", "correspondence address", "present address", "mailing address",
    "billing address", "shipping address", "registered address", "home address",
    "office address", "work address", "business address", "shop address", "delivery address", "native address",
    "house no", "building name", "flat no", "apartment", "door number", "plot no", "block",
    "floor", "tower", "unit number", "address line1", "address line2", "street", "street name",
    "road", "lane", "area", "locality", "colony", "sector", "village", "district",
    "taluk", "mandal", "tehsil", "municipality", "town", "city", "state", "region",
    "zone", "division", "province", "pincode", "pin", "postal code", "zip", "zip code",
    "location", "geo location", "place", "addr", "addr1", "addr2"
]
KEYWORD_PATTERN = re.compile(
    r"(?P<key>" + "|".join(re.escape(k) for k in address_keys) + r")\s*[:=]\s*(?P<val>[^,}\n\r]+)",
    flags=re.IGNORECASE
)

# === FLATTEN JSON ===
def flatten_json(obj, prefix=""):
    flat = {}
    if isinstance(obj, dict):
        for k, v in obj.items():
            path = f"{prefix}{k}"
            if isinstance(v, (dict, list)):
                flat.update(flatten_json(v, path + "."))
            else:
                flat[path] = v
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            flat.update(flatten_json(item, f"{prefix}[{i}]."))
    return flat

# === LOAD & GROUP RECORDS ===
def load_records():
    """
    Single-pass header + brace-scanning loader.
    Returns:
      records: list of (set_name, key, raw_json)
      warnings: count of malformed or unbalanced JSON blocks
      skipped: list of (set_name, key, reason)
    """
    text = open(INPUT_FILE, encoding="utf-8", errors="ignore").read()
    header_pat = re.compile(
        r"Set\s+Name\s*:\s*(?P<set>[^,]+)"
        r"(?:\s*,\s*Key\s*:\s*(?P<key>[^,]+))?"
        r".*?JSON\s*Data\s*:",
        flags=re.IGNORECASE
    )

    records, skipped, warnings = [], [], 0
    pos, L = 0, len(text)

    while True:
        m = header_pat.search(text, pos)
        if not m:
            break
        set_name = m.group("set").strip()
        key      = (m.group("key") or "").strip()
        brace_i  = text.find("{", m.end())
        if brace_i < 0:
            warnings += 1
            skipped.append((set_name, key, "no JSON opening brace"))
            pos = m.end()
            continue
        count, i = 1, brace_i + 1
        while i < L and count:
            if text[i] == "{":
                count += 1
            elif text[i] == "}":
                count -= 1
            i += 1
        if count != 0:
            warnings += 1
            skipped.append((set_name, key, "unbalanced JSON braces"))
            pos = m.end()
            continue
        raw_json = text[brace_i:i]
        records.append((set_name, key, raw_json))
        pos = i

    return records, warnings, skipped

# === SPLIT INTO CHUNKS ===
def chunk_records(records):
    for idx in range(0, len(records), CHUNK_SIZE):
        yield records[idx:idx+CHUNK_SIZE], idx//CHUNK_SIZE

# === PROCESS ONE CHUNK (ADDRESS EXTRACTION) ===
def process_chunk(args):
    chunk, idx, result_list = args

    doc = Document()
    rows = []
    seen = with_addr = hits = parse_fail = 0
    parse_errors = []

    for set_name, key, raw_json in chunk:
        seen += 1
        try:
            obj = json.loads(raw_json)
            flat = flatten_json(obj)
        except json.JSONDecodeError:
            parse_fail += 1
            parse_errors.append((set_name, key, raw_json))
            continue

        # 1) JSON key‐match
        json_matches = [
            (".".join(path.split(".")), str(v))
            for path, v in flat.items()
            if any(kw in path.lower() for kw in address_keys)
        ]
        # 2) regex fallback
        fallback = [
            (m.group("key").strip(), m.group("val").strip())
            for m in KEYWORD_PATTERN.finditer(raw_json)
        ]
        # Combine uniquely
        combined = []
        seen_pairs = set()
        for fn, val in json_matches + fallback:
            pair = (fn, val)
            if pair not in seen_pairs:
                seen_pairs.add(pair)
                combined.append(pair)

        if not combined:
            continue

        with_addr += 1
        para = doc.add_paragraph(f"{set_name} | {key} | ")
        para.add_run(raw_json.replace("\n", " "))

        for fn, val in combined:
            hits += 1
            run1 = para.add_run(f" | {val}")
            run1.font.color.rgb = RGBColor(255,0,0)
            run2 = para.add_run(f" | field: {fn}")
            run2.font.color.rgb = RGBColor(255,0,0)
            rows.append((set_name, key, raw_json, val, fn))

    if rows:
        os.makedirs(TEMP_DIR, exist_ok=True)
        doc.save(os.path.join(TEMP_DIR, f"chunk_{idx}.docx"))

    result_list.append((rows, seen, with_addr, hits, parse_fail, parse_errors))

# === MERGE DOCX CHUNKS ===
def merge_word():
    merged = Document()
    for fn in tqdm(sorted(os.listdir(TEMP_DIR)), desc="Merging Word"):
        if not fn.endswith(".docx"):
            continue
        sub = Document(os.path.join(TEMP_DIR, fn))
        for para in sub.paragraphs:
            out = merged.add_paragraph()
            for run in para.runs:
                nr = out.add_run(run.text)
                if run.font.color and run.font.color.rgb:
                    nr.font.color.rgb = run.font.color.rgb
                nr.bold, nr.italic, nr.underline = run.bold, run.italic, run.underline
    merged.save(OUTPUT_DOCX)

# === WRITE EXCEL (Matches + inline Parse Failures) ===
def write_excel(rows, parse_errors):
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    ws = wb.add_worksheet("Results")
    red = wb.add_format({"font_color":"red"})

    # Matches
    ws.write_row(0, 0, ["Set Name","Key","Full JSON","Value","Field"])
    r = 1
    for a,b,c,val,fn in rows:
        ws.write(r,0,a); ws.write(r,1,b); ws.write(r,2,c)
        ws.write(r,3,val,red); ws.write(r,4,fn)
        r += 1

    # Two blank lines
    r += 1

    # Parse Failures
    ws.write_row(r,0,["Parse Failures:"])
    r += 1
    ws.write_row(r,0,["Set Name","Key","Raw JSON"])
    r += 1
    for a,b,raw in parse_errors:
        ws.write(r,0,a); ws.write(r,1,b); ws.write(r,2,raw)
        r += 1

    wb.close()

if __name__=="__main__":
    # Load
    print("Loading Aerospike records …")
    records, warn_count, skipped = load_records()
    print(f"→ Total records loaded: {len(records)}  (warnings: {warn_count})")

    # Process
    mgr = Manager()
    results = mgr.list()
    chunks = list(chunk_records(records))
    with Pool(min(len(chunks), cpu_count())) as pool:
        list(tqdm(
            pool.imap_unordered(process_chunk,
                                [(chunk, idx, results) for chunk, idx in chunks]),
            total=len(chunks),
            desc="Processing records",
        ))

    # Aggregate
    all_rows = []
    total_seen = total_with_addr = total_hits = total_pf = 0
    parse_errors = []
    for rows, seen, with_addr, hits, pfail, pfd in results:
        all_rows.extend(rows)
        total_seen += seen
        total_with_addr += with_addr
        total_hits += hits
        total_pf += pfail
        parse_errors.extend(pfd)

    # Summary
    print(
        f"\nScanned {total_seen} records, "
        f"{total_with_addr} had address matches, "
        f"{total_hits} total hits, "
        f"{total_pf} JSON parse failures"
    )

    # Output
    if os.path.isdir(TEMP_DIR) and total_with_addr > 0:
        merge_word()
    write_excel(all_rows, parse_errors)
    if os.path.isdir(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)

    print(f"\n→ Word saved: {OUTPUT_DOCX}")
    print(f"→ Excel saved: {OUTPUT_XLSX}\n")
    sys.exit(0 if total_hits > 0 and total_pf == 0 else 1)
