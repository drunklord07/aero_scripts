import os
import re
import json
import shutil
import sys
from multiprocessing import Pool, Manager, cpu_count
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE   = "input.txt"
CHUNK_SIZE   = 2000
OUTPUT_DOCX  = "aerospike_name.docx"
OUTPUT_XLSX  = "aerospike_name.xlsx"
TEMP_DIR     = "temp_aero_name_parts"
name_keys    = ["name"]
KEYWORD_PATTERN = re.compile(
    r"(?P<key>" + "|".join(re.escape(k) for k in name_keys) + r")\s*[:=]\s*(?P<val>[^,}\n\r]+)",
    flags=re.IGNORECASE
)

# === FLATTEN JSON ===
def flatten_json(obj, prefix=""):
    flat = {}
    if isinstance(obj, dict):
        for k, v in obj.items():
            path = f"{prefix}{k}"
            if isinstance(v, (dict, list)):
                flat.update(flatten_json(v, path + "."))
            else:
                flat[path] = v
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            flat.update(flatten_json(item, f"{prefix}[{i}]."))
    return flat

# === LOAD & GROUP RECORDS ===
def load_records():
    """
    Single-pass header + brace-scanning loader.
    Returns records, warning count, skipped headers.
    """
    text = open(INPUT_FILE, encoding="utf-8", errors="ignore").read()
    header_pat = re.compile(
        r"Set\s+Name\s*:\s*(?P<set>[^,]+)"
        r"(?:\s*,\s*Key\s*:\s*(?P<key>[^,]+))?"
        r".*?JSON\s*Data\s*:",
        flags=re.IGNORECASE
    )

    records, skipped, warnings = [], [], 0
    pos, L = 0, len(text)
    while True:
        m = header_pat.search(text, pos)
        if not m:
            break
        set_name = m.group("set").strip()
        key      = (m.group("key") or "").strip()
        brace_i  = text.find("{", m.end())
        if brace_i < 0:
            warnings += 1
            skipped.append((set_name, key, "no JSON opening brace"))
            pos = m.end()
            continue
        count, i = 1, brace_i + 1
        while i < L and count:
            if text[i] == "{":
                count += 1
            elif text[i] == "}":
                count -= 1
            i += 1
        if count != 0:
            warnings += 1
            skipped.append((set_name, key, "unbalanced JSON braces"))
            pos = m.end()
            continue
        raw_json = text[brace_i:i]
        records.append((set_name, key, raw_json))
        pos = i
    return records, warnings, skipped

# === SPLIT INTO CHUNKS ===
def chunk_records(records):
    for idx in range(0, len(records), CHUNK_SIZE):
        yield records[idx:idx+CHUNK_SIZE], idx//CHUNK_SIZE

# === PROCESS ONE CHUNK (NAME EXTRACTION) ===
def process_chunk(args):
    chunk, idx, result_list = args
    doc = Document()
    rows = []
    seen = with_name = hits = parse_fail = 0
    parse_errors = []

    for set_name, key, raw_json in chunk:
        seen += 1
        try:
            obj = json.loads(raw_json)
            flat = flatten_json(obj)
        except json.JSONDecodeError:
            parse_fail += 1
            parse_errors.append((set_name, key, raw_json))
            continue

        # JSON key matches
        json_matches = [
            (".".join(path.split(".")), str(v))
            for path, v in flat.items()
            if any(kw in path.lower() for kw in name_keys)
        ]
        # regex fallback
        fallback = [
            (m.group("key").strip(), m.group("val").strip())
            for m in KEYWORD_PATTERN.finditer(raw_json)
        ]

        combined = []
        seen_pairs = set()
        for fn, val in json_matches + fallback:
            if (fn, val) not in seen_pairs:
                seen_pairs.add((fn, val))
                combined.append((fn, val))

        if not combined:
            continue

        with_name += 1
        para = doc.add_paragraph(f"{set_name} | {key} | ")
        para.add_run(raw_json.replace("\n", " "))

        for field_name, val_text in combined:
            hits += 1
            run1 = para.add_run(f" | {val_text}")
            run1.font.color.rgb = RGBColor(255,0,0)
            run2 = para.add_run(f" | field: {field_name}")
            run2.font.color.rgb = RGBColor(255,0,0)
            rows.append((set_name, key, raw_json, val_text, field_name))

    if rows:
        os.makedirs(TEMP_DIR, exist_ok=True)
        doc.save(os.path.join(TEMP_DIR, f"chunk_{idx}.docx"))

    result_list.append((rows, seen, with_name, hits, parse_fail, parse_errors))

# === MERGE DOCX CHUNKS ===
def merge_word():
    merged = Document()
    for fn in tqdm(sorted(os.listdir(TEMP_DIR)), desc="Merging Word"):
        if not fn.endswith(".docx"):
            continue
        sub = Document(os.path.join(TEMP_DIR, fn))
        for para in sub.paragraphs:
            out = merged.add_paragraph()
            for run in para.runs:
                nr = out.add_run(run.text)
                if run.font.color and run.font.color.rgb:
                    nr.font.color.rgb = run.font.color.rgb
                nr.bold, nr.italic, nr.underline = run.bold, run.italic, run.underline
    merged.save(OUTPUT_DOCX)

# === WRITE EXCEL (Matches + inline Parse Failures) ===
def write_excel(rows, parse_errors):
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    ws = wb.add_worksheet("Results")
    red = wb.add_format({"font_color":"red"})

    # Matches
    ws.write_row(0, 0, ["Set Name","Key","Full JSON","Value","Field"])
    r = 1
    for a, b, c, v, f in rows:
        ws.write(r,0,a); ws.write(r,1,b); ws.write(r,2,c)
        ws.write(r,3,v,red); ws.write(r,4,f)
        r += 1

    # Two blank lines
    r += 1

    # Parse Failures
    ws.write_row(r,0,["Parse Failures:"])
    r += 1
    ws.write_row(r,0,["Set Name","Key","Raw JSON"])
    r += 1
    for a, b, raw in parse_errors:
        ws.write(r,0,a); ws.write(r,1,b); ws.write(r,2,raw)
        r += 1

    wb.close()

if __name__=="__main__":
    print("Loading Aerospike records …")
    records, warn_count, skipped = load_records()
    print(f"→ Total records loaded: {len(records)}  (warnings: {warn_count})")

    mgr = Manager()
    results = mgr.list()
    chunks = list(chunk_records(records))
    with Pool(min(len(chunks), cpu_count())) as pool:
        list(tqdm(
            pool.imap_unordered(process_chunk,
                                [(chunk, idx, results) for chunk, idx in chunks]),
            total=len(chunks),
            desc="Processing records",
        ))

    all_rows = []
    total_seen = total_with = total_hits = total_pf = 0
    parse_errors = []
    for rows, seen, with_name, hits, pfail, pfd in results:
        all_rows.extend(rows)
        total_seen += seen
        total_with += with_name
        total_hits += hits
        total_pf += pfail
        parse_errors.extend(pfd)

    print(
        f"\nScanned {total_seen} records, "
        f"{total_with} had matches, "
        f"{total_hits} total hits, "
        f"{total_pf} JSON parse failures"
    )

    if os.path.isdir(TEMP_DIR) and total_with > 0:
        merge_word()
    write_excel(all_rows, parse_errors)
    shutil.rmtree(TEMP_DIR, ignore_errors=True)

    print(f"\n→ Word saved: {OUTPUT_DOCX}")
    print(f"→ Excel saved: {OUTPUT_XLSX}\n")
    sys.exit(0 if total_hits > 0 and total_pf == 0 else 1)
